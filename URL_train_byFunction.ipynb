{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 구조 \n",
    "\n",
    "scheme://사용자이름:비밀번호@호스트:포트/경로?질의#fragment\n",
    "\n",
    "URL 허용문자가 있다. 즉 안전한 전송을 위해서 초기 URL 설계자들이 알파벳(ASCII) 만으로 URL을 작성하도록 하였다. 그런데, 점점 커지면서 ASCII 외의 문자들을 이스케이프(escape)처리하였다. 즉 이스케이프 문자로 인코딩 함을 말한다. \n",
    "  예를들어 빈문자열은 \"%20\" 으로, ~ 는 \"%7\" 로 인코딩 된다. \n",
    "  \n",
    "  즉 https://ko.wikipedia.org/wiki/위키 는 https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4 \n",
    "  \n",
    "  이렇게 인코딩된다. \n",
    "  \n",
    "  \n",
    "  \n",
    "  ko.wikipedia.org는 서버가 된다. 원하는 파일은 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "url string을 넣으면, 원하는 part에 맞는 string을 return한다. \n",
    "'''\n",
    "\n",
    "def split_url(line, part):\n",
    "    if line.startswith(\"http://\"):\n",
    "        line=line[7:]\n",
    "    if line.startswith(\"https://\"):\n",
    "        line=line[8:]\n",
    "    if line.startswith(\"ftp://\"):\n",
    "        line=line[6:]\n",
    "    if line.startswith(\"www.\"):\n",
    "        line = line[4:]\n",
    "    # 가장 첫번째 / 는 primary domain과 path를 나눠주는 경계이다.     \n",
    "    slash_pos = line.find('/')\n",
    "    if slash_pos > 0 and slash_pos < len(line)-1: # line = \"fsdfsdf/sdfsdfsd\"\n",
    "        primarydomain = line[:slash_pos]\n",
    "        path_argument = line[slash_pos+1:]\n",
    "        \n",
    "        # path 부분이긴 한데, 이 부분 또한 / 로 세부적으로 나눠질 수가 있다. \n",
    "        path_argument_tokens = path_argument.split('/')\n",
    "        \n",
    "        # 마지막 부분을 제외하고, 그 이전까지의 string들은 join으로 합친다. (왜냐면 그냥 경로를 나타내는 것임) \n",
    "        pathtoken = \"/\".join(path_argument_tokens[:-1])\n",
    "        \n",
    "        # 마지막 부분은 path 이후의 부분과 구분하기 위해서 일부러 분리한다. \n",
    "        last_pathtoken = path_argument_tokens[-1]\n",
    "        \n",
    "        \n",
    "        if len(path_argument_tokens) > 2 and last_pathtoken == '':\n",
    "            pathtoken = \"/\".join(path_argument_tokens[:-2])\n",
    "            last_pathtoken = path_argument_tokens[-2]\n",
    "        question_pos = last_pathtoken.find('?')\n",
    "        if question_pos != -1:\n",
    "            argument = last_pathtoken[question_pos+1:]\n",
    "            pathtoken = pathtoken + \"/\" + last_pathtoken[:question_pos]     \n",
    "        else:\n",
    "            argument = \"\"\n",
    "            pathtoken = pathtoken + \"/\" + last_pathtoken          \n",
    "        last_slash_pos = pathtoken.rfind('/')\n",
    "        sub_dir = pathtoken[:last_slash_pos]\n",
    "        filename = pathtoken[last_slash_pos+1:]\n",
    "        file_last_dot_pos = filename.rfind('.')\n",
    "        if file_last_dot_pos != -1:\n",
    "            file_extension = filename[file_last_dot_pos+1:]\n",
    "            filename = filename[:file_last_dot_pos]\n",
    "        else:\n",
    "            file_extension = \"\" \n",
    "    elif slash_pos == 0:    # line = \"/fsdfsdfsdfsdfsd\"\n",
    "        primarydomain = line[1:]\n",
    "        pathtoken = \"\"\n",
    "        argument = \"\"\n",
    "        sub_dir = \"\"\n",
    "        filename = \"\"\n",
    "        file_extension = \"\"\n",
    "    elif slash_pos == len(line)-1:   # line = \"fsdfsdfsdfsdfsd/\"\n",
    "        primarydomain = line[:-1]\n",
    "        pathtoken = \"\"\n",
    "        argument = \"\"\n",
    "        sub_dir = \"\"     \n",
    "        filename = \"\"\n",
    "        file_extension = \"\"\n",
    "    else:      # line = \"fsdfsdfsdfsdfsd\"\n",
    "        primarydomain = line\n",
    "        pathtoken = \"\"\n",
    "        argument = \"\"\n",
    "        sub_dir = \"\" \n",
    "        filename = \"\"\n",
    "        file_extension = \"\"\n",
    "    if part == 'pd':\n",
    "        return primarydomain\n",
    "    elif part == 'path':\n",
    "        return pathtoken\n",
    "    elif part == 'argument': \n",
    "        return argument \n",
    "    elif part == 'sub_dir': \n",
    "        return sub_dir \n",
    "    elif part == 'filename': \n",
    "        return filename \n",
    "    elif part == 'fe': \n",
    "        return file_extension\n",
    "    elif part == 'others': \n",
    "        if len(argument) > 0: \n",
    "            return pathtoken + '?' +  argument \n",
    "        else: \n",
    "            return pathtoken \n",
    "    else:\n",
    "        return primarydomain, pathtoken, argument, sub_dir, filename, file_extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['path', 'to', 'server']\n",
      "path/to\n",
      "server\n"
     ]
    }
   ],
   "source": [
    "path_argument = \"path/to/server\"\n",
    "\n",
    "path_argument_tokens = path_argument.split('/')\n",
    "print(path_argument_tokens)\n",
    "\n",
    "pathtoken = \"/\".join(path_argument_tokens[:-1])\n",
    "print(pathtoken)        \n",
    "last_pathtoken = path_argument_tokens[-1]\n",
    "print(last_pathtoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이게 뭐하는거지? \n",
    "\n",
    "def get_word_vocab(urls, max_length_words, min_word_freq=0): \n",
    "    # 즉 url들이 모여있는 url목록에서, 고유한 단어들을 모두 추출한다.\n",
    "    # 단어들에 고유한 id를 보유한다. (보통 숫자형태로) \n",
    "    # 각각의 url들을 id로 표현한다. 이때, max_length_words보다 길이가 짧은 것들은 걍 0으로 채운다. \n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_length_words, min_frequency=min_word_freq) \n",
    "    start = time.time() \n",
    "    \n",
    "    # 즉, 각 url들을 단어 ID, padding으로 치환한 다음, 그 치환된것을 np.array에 넣은것.\n",
    "    x = np.array(list(vocab_processor.fit_transform(urls)))\n",
    "    print(\"Finished build vocabulary and mapping to x in {}\".format(time.time() - start))\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    \n",
    "    # key-value를 바꾼다. 원래는 단어가 key, 숫자가 value였는데, 숫자 id가 key, 단어가 value가 되게끔 바꾼다. \n",
    "    reverse_dict = dict(zip(vocab_dict.values(), vocab_dict.keys()))\n",
    "    print(\"Size of word vocabulary: {}\".format(len(reverse_dict)))\n",
    "    return x, reverse_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f923af35f4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabularyProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import learn \n",
    "learn.preprocessing.VocabularyProcessor(test_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위의 learn.preprocessing.VocabularyProcessor 는 deprecated  된단다. \n",
    "\n",
    "# 그래서 아래 코드로 대체하여 사용해보자 ? \n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<UNK>\")\n",
    "tokenizer.fit_on_texts(x_text)\n",
    "x = tokenizer.texts_to_sequences(x_text)\n",
    "\n",
    "x = tf.keraspreprocessing.sequence.pad_sequences(x, maxlen=max_document_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 여기에 들어가는 x는 URL이 숫자 id로 표현된 np array이다. \n",
    "# 보통 <UNK>가 0에 mapping 된다. <UNK> 는 ID 0을 갖고, 거의 언급이 안되는 애들을 뜻하는듯?\n",
    "\n",
    "# 이 함수의 return은 단어들과, 특수문자들은 구분하여... \n",
    "'''\n",
    "https://naver.com 이라면 \n",
    "https, :, /, /, naver, . , com 이런식으로 구분하는 것이다. \n",
    "'''\n",
    "def get_words(x, reverse_dict, delimit_mode, urls=None): \n",
    "    \n",
    "     processed_x = []\n",
    "    \n",
    "    # 각 url 마다 뒤에 빈 것들을 없애고, id가 아닌, string 값으로 있는것.\n",
    "    if delimit_mode == 0: \n",
    "        for url in x: \n",
    "            # words는 걍 string 단어의 집합? \n",
    "            words = []\n",
    "            for word_id in url: \n",
    "                if word_id != 0: \n",
    "                    words.append(reverse_dict[word_id])\n",
    "                # 즉 해당 url이 모두 끝났다는 말임. \n",
    "                else: \n",
    "                    break\n",
    "                    \n",
    "            processed_x.append(words) \n",
    "    \n",
    "    # \n",
    "    elif delimit_mode == 1: \n",
    "        for i in range(x.shape[0]):\n",
    "            word_url = x[i]\n",
    "            raw_url = urls[i]\n",
    "            words = []\n",
    "            for w in range(len(word_url)): \n",
    "                word_id = word_url[w]\n",
    "                # 0이라는건, 이제 URL이 끝이다. 따라서. raw_url을 그냥 넣는다. \n",
    "                #(어차피 아무것도 없을거니깐) 그리고 break\n",
    "                if word_id == 0: \n",
    "                    words.extend(list(raw_url))\n",
    "                    break\n",
    "                else: \n",
    "                    # 즉 끝이 아니라, 어떤 특정단어에 mapping이 되었다.     \n",
    "                    word = reverse_dict[word_id]\n",
    "                    idx = raw_url.index(word) \n",
    "                    special_chars = list(raw_url[0:idx])\n",
    "                    words.extend(special_chars) \n",
    "                    words.append(word) \n",
    "                    # 그 일반 단어 그 다음부터 세게끔 해서.. \n",
    "                    raw_url = raw_url[idx+len(word):]\n",
    "                    if w == len(word_url) - 1: \n",
    "                        words.extend(list(raw_url))\n",
    "            processed_x.append(words)\n",
    "    return processed_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "urls, labels = read_data(FLAGS[\"data.data_dir\"]) \n",
    "\n",
    "# FLAGS[\"data.max_len_words\"] 는 URL 당 최개 가질 수 있는 단어의 갯수 \n",
    "\n",
    "# x는 url들을 id의 모음으로 나타낸것, word_reverse_dict는 단어-id 매핑정보 \n",
    "x, word_reverse_dict = get_word_vocab(urls, FLAGS[\"data.max_len_words\"])\n",
    "\n",
    "# 특수문자는 미포함하여, [https, naver, com] 이면 delimite code =1 \n",
    "# 특수문자 포함할거면, [https, :, /, /, naver, ., com] 이런식으로 표현가능. \n",
    "word_x = get_words(x, word_reverse_dict, FLAGS[\"data.delimit_mode\"], urls)\n",
    "\n",
    "\n",
    "ngramed_id_x, ngrams_dict, worded_id_x, words_dict = ngram_id_x(word_x, FLAGS[\"data.max_len_subwords\"], high_freq_words)\n",
    "\n",
    "def ngram_id_x(word_x, max_len_subwords, high_freq_words=None):   \n",
    "    char_ngram_len = 1\n",
    "    all_ngrams = set() \n",
    "    ngramed_x = []\n",
    "    all_words = set() s in each word in a URL. Each word is either truncated or padded with a `<PADDING>` character to reach this length.                                            \n",
    "    worded_x = []\n",
    "    counter = 0\n",
    "    for url in word_x:\n",
    "        if counter % 100000 == 0: \n",
    "            print(\"Processing #url {}\".format(counter))\n",
    "        counter += 1  \n",
    "        url_in_ngrams = []\n",
    "        url_in_words = []\n",
    "        words = urls in each word in a URL. Each word is either truncated or padded with a `<PADDING>` character to reach this length.                                            \n",
    "        for word in words:\n",
    "            ngrams = get_char_ngrams(char_ngram_len, word) \n",
    "            if (len(ngrams) > max_len_subwords) or \\\n",
    "                (high_freq_words is not None and len(word)>1 and not is_in(high_freq_words, word)):  \n",
    "                all_ngrams.update(ngrams[:max_len_subwords])\n",
    "                url_in_ngrams.append(ngrams[:max_len_subwords]) \n",
    "                all_words.add(\"<UNKNOWN>\")\n",
    "                url_in_words.append(\"<UNKNOWN>\")\n",
    "            else:     \n",
    "                all_ngrams.update(ngrams)\n",
    "                url_in_ngrams.append(ngrams) \n",
    "                all_words.add(word) \n",
    "                url_in_words.append(word) \n",
    "        ngramed_x.append(url_in_ngrams)\n",
    "        worded_x.append(url_in_words) \n",
    "\n",
    "    all_ngrams = list(all_ngrams) \n",
    "    ngrams_dict = dict()\n",
    "    for i in range(len(all_ngrams)):  \n",
    "        ngrams_dict[all_ngrams[i]] = i+1 # ngram id=0 is for padding ngram   \n",
    "    print(\"Size of ngram vocabulary: {}\".format(len(ngrams_dict))) \n",
    "    all_words = list(all_words) \n",
    "    words_dict = dict() \n",
    "    for i in range(len(all_words)): \n",
    "        words_dict[all_words[i]] = i+1 #word id=0 for padding word \n",
    "    print(\"Size of word vocabulary: {}\".format(len(words_dict)))\n",
    "    print(\"Index of <UNKNOWN> word: {}\".format(words_dict[\"<UNKNOWN>\"]))        \n",
    "\n",
    "    ngramed_id_x = []\n",
    "    for ngramed_url in ngramed_x: \n",
    "        url_in_ngrams = []\n",
    "        for ngramed_word in ngramed_url: \n",
    "            ngram_ids = [ngrams_dict[x] for x in ngramed_word] \n",
    "            url_in_ngrams.append(ngram_ids) \n",
    "        ngramed_id_x.append(url_in_ngrams)  \n",
    "    worded_id_x = []\n",
    "    for worded_url in worded_x: \n",
    "        word_ids = [words_dict[x] for x in worded_url]\n",
    "        worded_id_x.append(word_ids) \n",
    "    \n",
    "    return ngramed_id_x, ngrams_dict, worded_id_x, words_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

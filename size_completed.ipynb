{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word_vocab(urls, max_length_words, min_word_freq=0): \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_length_words, min_frequency=min_word_freq) \n",
    "    x = np.array(list(vocab_processor.fit_transform(urls)))\n",
    "    vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "    reverse_dict = dict(zip(vocab_dict.values(), vocab_dict.keys()))\n",
    "    print(\"Size of word vocabulary: {}\".format(len(reverse_dict)))\n",
    "    return x, reverse_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-aa8c14a2a987>:6: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /home/youngjai/pythonVenv/tensorflow1.8/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n"
     ]
    }
   ],
   "source": [
    "max_length_words = 10\n",
    "min_word_freq = 0\n",
    "\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_length_words, \n",
    "                                                          min_frequency=min_word_freq)\n",
    "\n",
    "\n",
    "urls = [\"https://naver.com\", \"https://docs.ahnlab.com/ailab?df\", \"https://phishing\", \"ftp://dfsd.youngjai.juhee.com/love\"]\n",
    "x = np.array(list(vocab_processor.fit_transform(urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  5,  3,  6,  7,  0,  0,  0,  0],\n",
       "       [ 1,  8,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 9, 10, 11, 12,  3, 13,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'https': 1, 'naver': 2, 'com': 3, 'docs': 4, 'ahnlab': 5, 'ailab': 6, 'df': 7, 'phishing': 8, 'ftp': 9, 'dfsd': 10, 'youngjai': 11, 'juhee': 12, 'love': 13}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#<UNK>가 나오는데, 이는,,,, 채워지는게 없을 때 그렇게 나온다. \n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "print(vocab_dict)    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<UNK>', 1: 'https', 2: 'naver', 3: 'com', 4: 'docs', 5: 'ahnlab', 6: 'ailab', 7: 'df', 8: 'phishing', 9: 'ftp', 10: 'dfsd', 11: 'youngjai', 12: 'juhee', 13: 'love'}\n"
     ]
    }
   ],
   "source": [
    "reverse_dict = dict(zip(vocab_dict.values(), vocab_dict.keys()))\n",
    "print(reverse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_url: [1 2 3 0 0 0 0 0 0 0]\n",
      "raw_url: https://naver.com\n",
      "word_id: 1\n",
      "word: https\n",
      "in rawURL idx: 0\n",
      "special_chars []\n",
      "words ['https']\n",
      "word_id: 2\n",
      "word: naver\n",
      "in rawURL idx: 3\n",
      "special_chars [':', '/', '/']\n",
      "words ['https', ':', '/', '/', 'naver']\n",
      "word_id: 3\n",
      "word: com\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['https', ':', '/', '/', 'naver', '.', 'com']\n",
      "word_id: 0\n",
      "word_url: [1 4 5 3 6 7 0 0 0 0]\n",
      "raw_url: https://docs.ahnlab.com/ailab?df\n",
      "word_id: 1\n",
      "word: https\n",
      "in rawURL idx: 0\n",
      "special_chars []\n",
      "words ['https']\n",
      "word_id: 4\n",
      "word: docs\n",
      "in rawURL idx: 3\n",
      "special_chars [':', '/', '/']\n",
      "words ['https', ':', '/', '/', 'docs']\n",
      "word_id: 5\n",
      "word: ahnlab\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['https', ':', '/', '/', 'docs', '.', 'ahnlab']\n",
      "word_id: 3\n",
      "word: com\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['https', ':', '/', '/', 'docs', '.', 'ahnlab', '.', 'com']\n",
      "word_id: 6\n",
      "word: ailab\n",
      "in rawURL idx: 1\n",
      "special_chars ['/']\n",
      "words ['https', ':', '/', '/', 'docs', '.', 'ahnlab', '.', 'com', '/', 'ailab']\n",
      "word_id: 7\n",
      "word: df\n",
      "in rawURL idx: 1\n",
      "special_chars ['?']\n",
      "words ['https', ':', '/', '/', 'docs', '.', 'ahnlab', '.', 'com', '/', 'ailab', '?', 'df']\n",
      "word_id: 0\n",
      "word_url: [1 8 0 0 0 0 0 0 0 0]\n",
      "raw_url: https://phishing\n",
      "word_id: 1\n",
      "word: https\n",
      "in rawURL idx: 0\n",
      "special_chars []\n",
      "words ['https']\n",
      "word_id: 8\n",
      "word: phishing\n",
      "in rawURL idx: 3\n",
      "special_chars [':', '/', '/']\n",
      "words ['https', ':', '/', '/', 'phishing']\n",
      "word_id: 0\n",
      "word_url: [ 9 10 11 12  3 13  0  0  0  0]\n",
      "raw_url: ftp://dfsd.youngjai.juhee.com/love\n",
      "word_id: 9\n",
      "word: ftp\n",
      "in rawURL idx: 0\n",
      "special_chars []\n",
      "words ['ftp']\n",
      "word_id: 10\n",
      "word: dfsd\n",
      "in rawURL idx: 3\n",
      "special_chars [':', '/', '/']\n",
      "words ['ftp', ':', '/', '/', 'dfsd']\n",
      "word_id: 11\n",
      "word: youngjai\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['ftp', ':', '/', '/', 'dfsd', '.', 'youngjai']\n",
      "word_id: 12\n",
      "word: juhee\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['ftp', ':', '/', '/', 'dfsd', '.', 'youngjai', '.', 'juhee']\n",
      "word_id: 3\n",
      "word: com\n",
      "in rawURL idx: 1\n",
      "special_chars ['.']\n",
      "words ['ftp', ':', '/', '/', 'dfsd', '.', 'youngjai', '.', 'juhee', '.', 'com']\n",
      "word_id: 13\n",
      "word: love\n",
      "in rawURL idx: 1\n",
      "special_chars ['/']\n",
      "words ['ftp', ':', '/', '/', 'dfsd', '.', 'youngjai', '.', 'juhee', '.', 'com', '/', 'love']\n",
      "word_id: 0\n"
     ]
    }
   ],
   "source": [
    "processed_x = []\n",
    "for i in range(x.shape[0]):\n",
    "    word_url = x[i]\n",
    "    raw_url = urls[i]\n",
    "    print(\"word_url:\",word_url)\n",
    "    print(\"raw_url:\", raw_url)\n",
    "    words = []\n",
    "    \n",
    "    for w in range(len(word_url)):\n",
    "        word_id = word_url[w]\n",
    "        print(\"word_id:\",word_id)\n",
    "        if word_id == 0:\n",
    "            words.extend(list(raw_url))\n",
    "            break\n",
    "        else:\n",
    "            word = reverse_dict[word_id]\n",
    "            print(\"word:\",word)\n",
    "            idx = raw_url.index(word)\n",
    "            print(\"in rawURL idx:\",idx)\n",
    "            special_chars = list(raw_url[0:idx])\n",
    "            print(\"special_chars\", special_chars)\n",
    "            words.extend(special_chars)\n",
    "            words.append(word)\n",
    "            print(\"words\",words)\n",
    "            \n",
    "            raw_url = raw_url[idx+len(word):]\n",
    "            if w == len(word_url)-1:\n",
    "                words.extend(list(raw_url))\n",
    "    processed_x.append(words)          \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "reverse_dict 는 숫자가 key이고, 해당 string이 value인 경우.\n",
    "delimit mode는? \n",
    "0: delimit URL by special characters\n",
    "1: delimit URL by special characters && treat each special characters as words\n",
    "default로는 1이 설정되어 있다. \n",
    "\n",
    "get_words 라는 함수는 말그대로 URL에서 단어들(https, naver) 와 같이 특수문자로 구분되어서\n",
    "얻을 수 있는 단어들, 그리고 그 특수문자들 개개를 word로 받아서 저장하는 것이다. \n",
    "'''\n",
    "\n",
    "def get_words(x, reverse_dict, delimit_mode, urls=None): \n",
    "    \n",
    "    processed_x = []\n",
    "    if delimit_mode == 0: \n",
    "        for url in x: \n",
    "            # words는 걍 string 단어의 집합? \n",
    "            words = []\n",
    "            for word_id in url: \n",
    "                if word_id != 0: \n",
    "                    words.append(reverse_dict[word_id])\n",
    "                # 즉 해당 url이 모두 끝났다는 말임. \n",
    "                else: \n",
    "                    break\n",
    "                    \n",
    "            processed_x.append(words) \n",
    "    \n",
    "    # 특수문자를 기준으로 나누되, 특수문자도 words로 간주한다. \n",
    "    elif delimit_mode == 1: \n",
    "        for i in range(x.shape[0]):\n",
    "            word_url = x[i]     # 한 URL이 ID로 표현된것. \n",
    "            raw_url = urls[i]\n",
    "            words = []\n",
    "            for w in range(len(word_url)): \n",
    "                word_id = word_url[w]\n",
    "                \n",
    "                # 0 이라는건 padding된것. 존재하지 않으니까, string으로 된 URL을 단어로 쪼갠 (raw_url)\n",
    "                # 을 그냥 바로 words에 추가시킨다. \n",
    "                if word_id == 0: \n",
    "                    words.extend(list(raw_url))\n",
    "                    break\n",
    "                else: \n",
    "                    # 즉 끝이 아니라, 어떤 특정단어에 mapping이 되었다.     \n",
    "                    word = reverse_dict[word_id]\n",
    "                    idx = raw_url.index(word) # 해당단어가 몇번째인지 알아야.. \n",
    "                    special_chars = list(raw_url[0:idx])\n",
    "                    words.extend(special_chars) # 특수문자도 word로 간주하기 위해서 \n",
    "                    words.append(word) # 원래 word는 그냥 넣는것이고 \n",
    "                    \n",
    "                    # 그 일반 단어 그 다음부터 세게끔 해서.. \n",
    "                    raw_url = raw_url[idx+len(word):]\n",
    "                    if w == len(word_url) - 1: \n",
    "                        words.extend(list(raw_url))\n",
    "            processed_x.append(words)\n",
    "    return processed_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['https', ':', '/', '/', 'naver', '.', 'com'], ['https', ':', '/', '/', 'docs', '.', 'ahnlab', '.', 'com', '/', 'ailab', '?', 'df'], ['https', ':', '/', '/', 'phishing'], ['ftp', ':', '/', '/', 'dfsd', '.', 'youngjai', '.', 'juhee', '.', 'com', '/', 'love']]\n"
     ]
    }
   ],
   "source": [
    "temp = get_words(x, reverse_dict, 1, urls)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word vocabulary: 3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d5abdfe4ef33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_reverse_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_reverse_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "word_reverse_dict = get_word_vocab(urls, 10,1)\n",
    "sorted(list(word_reverse_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', 'com', 'https']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(word_reverse_dict[1].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'a', 'h', 'n', 'l', 'a', 'b', '>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "주어진 word에서 ngram_len 만큼씩 substring을 만들고, 한칸씩 sliding window 하는것 \n",
    "파악한다. \n",
    "'''\n",
    "def get_char_ngrams(ngram_len, word): \n",
    "    word = \"<\" + word + \">\" \n",
    "    chars = list(word) \n",
    "    begin_idx = 0\n",
    "    ngrams = []\n",
    "    while (begin_idx + ngram_len) <= len(chars): \n",
    "        end_idx = begin_idx + ngram_len \n",
    "        ngrams.append(\"\".join(chars[begin_idx:end_idx])) \n",
    "        begin_idx += 1 \n",
    "    return ngrams \n",
    "\n",
    "tempList = get_char_ngrams(1,\"ahnlab\")\n",
    "tempList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https', ':', '/', '/', 'naver', '.', 'com'],\n",
       " ['https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'docs',\n",
       "  '.',\n",
       "  'ahnlab',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'ailab',\n",
       "  '?',\n",
       "  'df'],\n",
       " ['https', ':', '/', '/', 'phishing'],\n",
       " ['ftp',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'dfsd',\n",
       "  '.',\n",
       "  'youngjai',\n",
       "  '.',\n",
       "  'juhee',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'love']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: https\n",
      "ng: ['<', 'h', 't', 't', 'p', 's', '>']\n",
      "word: :\n",
      "ng: ['<', ':', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: naver\n",
      "ng: ['<', 'n', 'a', 'v', 'e', 'r', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: com\n",
      "ng: ['<', 'c', 'o', 'm', '>']\n",
      "word: https\n",
      "ng: ['<', 'h', 't', 't', 'p', 's', '>']\n",
      "word: :\n",
      "ng: ['<', ':', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: docs\n",
      "ng: ['<', 'd', 'o', 'c', 's', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: ahnlab\n",
      "ng: ['<', 'a', 'h', 'n', 'l', 'a', 'b', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: com\n",
      "ng: ['<', 'c', 'o', 'm', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: ailab\n",
      "ng: ['<', 'a', 'i', 'l', 'a', 'b', '>']\n",
      "word: ?\n",
      "ng: ['<', '?', '>']\n",
      "word: df\n",
      "ng: ['<', 'd', 'f', '>']\n",
      "word: https\n",
      "ng: ['<', 'h', 't', 't', 'p', 's', '>']\n",
      "word: :\n",
      "ng: ['<', ':', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: phishing\n",
      "ng: ['<', 'p', 'h', 'i', 's', 'h', 'i', 'n', 'g', '>']\n",
      "word: ftp\n",
      "ng: ['<', 'f', 't', 'p', '>']\n",
      "word: :\n",
      "ng: ['<', ':', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: dfsd\n",
      "ng: ['<', 'd', 'f', 's', 'd', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: youngjai\n",
      "ng: ['<', 'y', 'o', 'u', 'n', 'g', 'j', 'a', 'i', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: juhee\n",
      "ng: ['<', 'j', 'u', 'h', 'e', 'e', '>']\n",
      "word: .\n",
      "ng: ['<', '.', '>']\n",
      "word: com\n",
      "ng: ['<', 'c', 'o', 'm', '>']\n",
      "word: /\n",
      "ng: ['<', '/', '>']\n",
      "word: love\n",
      "ng: ['<', 'l', 'o', 'v', 'e', '>']\n"
     ]
    }
   ],
   "source": [
    "ngramed_x = []\n",
    "worded_x = []\n",
    "all_ngrams = set()\n",
    "url_in_ngrams = []\n",
    "url_in_words = []\n",
    "word_x = temp\n",
    "for url in word_x:\n",
    "    words = url\n",
    "    for word in words:\n",
    "        print(\"word:\",word)\n",
    "        ngrams = get_char_ngrams(1, word)\n",
    "        print(\"ng:\",ngrams)\n",
    "        url_in_ngrams.append(ngrams) \n",
    "        url_in_words.append(word) \n",
    "        all_ngrams.update(ngrams)\n",
    "    ngramed_x.append(url_in_ngrams)\n",
    "    worded_x.append(url_in_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'naver',\n",
       "  '.',\n",
       "  'com',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'docs',\n",
       "  '.',\n",
       "  'ahnlab',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'ailab',\n",
       "  '?',\n",
       "  'df',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'phishing',\n",
       "  'ftp',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'dfsd',\n",
       "  '.',\n",
       "  'youngjai',\n",
       "  '.',\n",
       "  'juhee',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'love'],\n",
       " ['https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'naver',\n",
       "  '.',\n",
       "  'com',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'docs',\n",
       "  '.',\n",
       "  'ahnlab',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'ailab',\n",
       "  '?',\n",
       "  'df',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'phishing',\n",
       "  'ftp',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'dfsd',\n",
       "  '.',\n",
       "  'youngjai',\n",
       "  '.',\n",
       "  'juhee',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'love'],\n",
       " ['https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'naver',\n",
       "  '.',\n",
       "  'com',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'docs',\n",
       "  '.',\n",
       "  'ahnlab',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'ailab',\n",
       "  '?',\n",
       "  'df',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'phishing',\n",
       "  'ftp',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'dfsd',\n",
       "  '.',\n",
       "  'youngjai',\n",
       "  '.',\n",
       "  'juhee',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'love'],\n",
       " ['https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'naver',\n",
       "  '.',\n",
       "  'com',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'docs',\n",
       "  '.',\n",
       "  'ahnlab',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'ailab',\n",
       "  '?',\n",
       "  'df',\n",
       "  'https',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'phishing',\n",
       "  'ftp',\n",
       "  ':',\n",
       "  '/',\n",
       "  '/',\n",
       "  'dfsd',\n",
       "  '.',\n",
       "  'youngjai',\n",
       "  '.',\n",
       "  'juhee',\n",
       "  '.',\n",
       "  'com',\n",
       "  '/',\n",
       "  'love']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'n', 'a', 'v', 'e', 'r', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'o', 'c', 's', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'a', 'h', 'n', 'l', 'a', 'b', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'a', 'i', 'l', 'a', 'b', '>'],\n",
       "  ['<', '?', '>'],\n",
       "  ['<', 'd', 'f', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'p', 'h', 'i', 's', 'h', 'i', 'n', 'g', '>'],\n",
       "  ['<', 'f', 't', 'p', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'f', 's', 'd', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'y', 'o', 'u', 'n', 'g', 'j', 'a', 'i', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'j', 'u', 'h', 'e', 'e', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'l', 'o', 'v', 'e', '>']],\n",
       " [['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'n', 'a', 'v', 'e', 'r', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'o', 'c', 's', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'a', 'h', 'n', 'l', 'a', 'b', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'a', 'i', 'l', 'a', 'b', '>'],\n",
       "  ['<', '?', '>'],\n",
       "  ['<', 'd', 'f', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'p', 'h', 'i', 's', 'h', 'i', 'n', 'g', '>'],\n",
       "  ['<', 'f', 't', 'p', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'f', 's', 'd', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'y', 'o', 'u', 'n', 'g', 'j', 'a', 'i', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'j', 'u', 'h', 'e', 'e', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'l', 'o', 'v', 'e', '>']],\n",
       " [['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'n', 'a', 'v', 'e', 'r', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'o', 'c', 's', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'a', 'h', 'n', 'l', 'a', 'b', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'a', 'i', 'l', 'a', 'b', '>'],\n",
       "  ['<', '?', '>'],\n",
       "  ['<', 'd', 'f', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'p', 'h', 'i', 's', 'h', 'i', 'n', 'g', '>'],\n",
       "  ['<', 'f', 't', 'p', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'f', 's', 'd', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'y', 'o', 'u', 'n', 'g', 'j', 'a', 'i', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'j', 'u', 'h', 'e', 'e', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'l', 'o', 'v', 'e', '>']],\n",
       " [['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'n', 'a', 'v', 'e', 'r', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'o', 'c', 's', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'a', 'h', 'n', 'l', 'a', 'b', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'a', 'i', 'l', 'a', 'b', '>'],\n",
       "  ['<', '?', '>'],\n",
       "  ['<', 'd', 'f', '>'],\n",
       "  ['<', 'h', 't', 't', 'p', 's', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'p', 'h', 'i', 's', 'h', 'i', 'n', 'g', '>'],\n",
       "  ['<', 'f', 't', 'p', '>'],\n",
       "  ['<', ':', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'd', 'f', 's', 'd', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'y', 'o', 'u', 'n', 'g', 'j', 'a', 'i', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'j', 'u', 'h', 'e', 'e', '>'],\n",
       "  ['<', '.', '>'],\n",
       "  ['<', 'c', 'o', 'm', '>'],\n",
       "  ['<', '/', '>'],\n",
       "  ['<', 'l', 'o', 'v', 'e', '>']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word vocabulary: 3\n",
      "Number of words with freq >=1: 3\n",
      "Size of word vocabulary: 14\n",
      "Processing #url 0\n",
      "Size of ngram vocabulary: 27\n",
      "Size of word vocabulary: 7\n",
      "Index of <UNKNOWN> word: 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 즉 x가 a 안에 포함이 된다면 true 반환. \n",
    "def is_in(a,x): \n",
    "    i = bisect_left(a,x)\n",
    "    if i != len(a) and a[i] == x: \n",
    "        return True \n",
    "    else: # 범위 바깥이다. \n",
    "        return False \n",
    "\n",
    "\n",
    "    \n",
    "# x 배열에서 a의 위치를 찾아준다.     \n",
    "def bisect_left(a, x, lo=0, hi=None):\n",
    "    \"\"\"Return the index where to insert item x in list a, assuming a is sorted.\n",
    "\n",
    "    The return value i is such that all e in a[:i] have e < x, and all e in\n",
    "    a[i:] have e >= x.  So if x already appears in the list, a.insert(x) will\n",
    "    insert just before the leftmost x already there.\n",
    "\n",
    "    Optional args lo (default 0) and hi (default len(a)) bound the\n",
    "    slice of a to be searched.\n",
    "    \"\"\"\n",
    "\n",
    "    # 즉 a가 문자열로 정렬되어 있다고 가정했을 때, 적당한 위치를 x가 들어가려고.. 들어갈 수 있는 \n",
    "    # 위치를 return한다. \n",
    "    if lo < 0:\n",
    "        raise ValueError('lo must be non-negative')\n",
    "    if hi is None:\n",
    "        hi = len(a)\n",
    "    while lo < hi:\n",
    "        mid = (lo+hi)//2\n",
    "        if a[mid] < x: lo = mid+1\n",
    "        else: hi = mid\n",
    "    return lo\n",
    "\n",
    "\n",
    "\n",
    "def get_char_ngrams(ngram_len, word): \n",
    "    word = \"<\" + word + \">\" \n",
    "    chars = list(word) \n",
    "    begin_idx = 0\n",
    "    ngrams = []\n",
    "    while (begin_idx + ngram_len) <= len(chars): \n",
    "        end_idx = begin_idx + ngram_len \n",
    "        ngrams.append(\"\".join(chars[begin_idx:end_idx])) \n",
    "        begin_idx += 1 \n",
    "    return ngrams \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "high_freq_words = None\n",
    "\n",
    "min_word_freq >0 이라는 것은,,,,\n",
    "만약 1이라면, freq가 1이거나 그것보다 작으면 걍 <unknown>으로 다 대체하겠다는 의미임. \n",
    "단어의 출현빈도 횟수가 특정 숫자보다 낮으면 다 퉁쳐버리겠다는 단어임. \n",
    "\n",
    "if FLAGS[\"data.min_word_freq\"] > 0: \n",
    "    x1, word_reverse_dict = get_word_vocab(urls, FLAGS[\"data.max_len_words\"], FLAGS[\"data.min_word_freq\"]) \n",
    "    high_freq_words = sorted(list(word_reverse_dict.values()))\n",
    "    print(\"Number of words with freq >={}: {}\".format(FLAGS[\"data.min_word_freq\"], len(high_freq_words)))  \n",
    "\n",
    "즉 min_word_freq 이상인 모든 단어들의 집합을 말한다. \n",
    "\n",
    "word_x : [[\"https\", \":\", \"/\", \"naver\", \".\", \"com\"], [\"https:\", ... ]]\n",
    "max_len_subwords : maximum number of words in a URL. 이걸 넘기면 짤리고, 못넘기면, <PADDING>으로 채워진다.\n",
    "'''\n",
    "def ngram_id_x(word_x, max_len_subwords, high_freq_words=None):   \n",
    "    char_ngram_len = 1\n",
    "    all_ngrams = set() \n",
    "    ngramed_x = []  # 각 URL -> 각 단어 -> 단어의 char 별로 seq로 묶여있다. \n",
    "    all_words = set() \n",
    "    worded_x = []\n",
    "    counter = 0\n",
    "    for url in word_x:\n",
    "        if counter % 100000 == 0: \n",
    "            print(\"Processing #url {}\".format(counter))\n",
    "        counter += 1  \n",
    "        url_in_ngrams = []\n",
    "        url_in_words = []\n",
    "        words = url # 각 url이 단어들의 seq로 표현되어 있으므로\n",
    "        for word in words:\n",
    "            # 각 단어에서 글자 하나만 가지고 있는 것.\n",
    "            ngrams = get_char_ngrams(char_ngram_len, word) \n",
    "            \n",
    "            # 별루 언급되지 않은 단어 => not is_in(high_freq_words, word) 로 표현한다.\n",
    "            # 그 때는 <UNKNOWN>으로 대체해서 넣는다. \n",
    "            # max_len_subwords보다 길면, truncated해서 넣어야 되니까\n",
    "            if (len(ngrams) > max_len_subwords) or \\\n",
    "                (high_freq_words is not None and len(word)>1 and not is_in(high_freq_words, word)):  \n",
    "                all_ngrams.update(ngrams[:max_len_subwords])\n",
    "                url_in_ngrams.append(ngrams[:max_len_subwords]) \n",
    "                all_words.add(\"<UNKNOWN>\")\n",
    "                url_in_words.append(\"<UNKNOWN>\")\n",
    "            else:     \n",
    "                all_ngrams.update(ngrams)\n",
    "                url_in_ngrams.append(ngrams) \n",
    "                all_words.add(word) \n",
    "                url_in_words.append(word)\n",
    "        # 한 URL의 단어를 각 글자로 쪼개버린 걸 담는다.\n",
    "        # 위처럼 한 걸 URL마다 담음. \n",
    "        ngramed_x.append(url_in_ngrams)\n",
    "        \n",
    "        ## 한 URL의 단어 그 자체를 담는다. worded_x에다가  \n",
    "        worded_x.append(url_in_words) \n",
    "\n",
    "    ## ngramed_x : 모든 URL 목록들을 각각 쪼갠다음, 각 URL을 ngram단위로 한것들의 집합.\n",
    "    ## all_ngrams 는 전체 URL 목록에 대해서 중복되지 않은 ngrams들을 뽑아낸것. 즉 https://~ 라면, 각각의 1개짜리 ngram은 h,t,p이니까.. 이것들에 id를 부여한 것이다. \n",
    "    all_ngrams = list(all_ngrams) \n",
    "    \n",
    "    # 1글자씩 쪼개버린걸 dict로 바꿔서 넣는다. \n",
    "    ngrams_dict = dict()\n",
    "    for i in range(len(all_ngrams)):  \n",
    "        ngrams_dict[all_ngrams[i]] = i+1 # ngram id=0 is for padding ngram   \n",
    "        \n",
    "    # 즉 url목록에 있는 모든 글자가 이루어진 글자의 갯수     \n",
    "    print(\"Size of ngram vocabulary: {}\".format(len(ngrams_dict))) \n",
    "    \n",
    "    # words를 이루는 character 한 개가 아니라, 그냥 단어별로 넣는다.\n",
    "    all_words = list(all_words) \n",
    "    words_dict = dict() \n",
    "    for i in range(len(all_words)): \n",
    "        words_dict[all_words[i]] = i+1 #word id=0 for padding word \n",
    "    print(\"Size of word vocabulary: {}\".format(len(words_dict)))\n",
    "    print(\"Index of <UNKNOWN> word: {}\".format(words_dict[\"<UNKNOWN>\"]))        \n",
    "\n",
    "    \n",
    "    ngramed_id_x = []\n",
    "    \n",
    "    # ngramed_x는 [['h','t','t','p',],[':'],['/']..[]] , [['h']..[]] ]\n",
    "    for ngramed_url in ngramed_x: \n",
    "        url_in_ngrams = []\n",
    "        \n",
    "        # 한 URL에서 (1짜리 ngram으로 단어별로 나눠진것. )\n",
    "        # 따라서 ngramed_word는 URL에 속한 단어가 한 글자씩으로 쪼개진 것을 의미한다. \n",
    "        for ngramed_word in ngramed_url: \n",
    "            \n",
    "            # x는 지금 한 글자임.(ngram을 1로해서...) => 그리고 그 ngram의 id를 얻으려고, dict랑 mapping한다.\n",
    "            ngram_ids = [ngrams_dict[x] for x in ngramed_word] \n",
    "            \n",
    "            # 그리고 그 주어진 단어(하나의 URL을 이루는 단어)를 ngram의 id로 표현했다.\n",
    "            url_in_ngrams.append(ngram_ids)\n",
    "        \n",
    "        # 위 loop가 끝나면 한 URL을 이루는 단어가 숫자 id로 mapping되어 나타난다.\n",
    "        ngramed_id_x.append(url_in_ngrams)  \n",
    "        \n",
    "    worded_id_x = []\n",
    "    for worded_url in worded_x: \n",
    "        word_ids = [words_dict[x] for x in worded_url]\n",
    "        worded_id_x.append(word_ids) \n",
    "    \n",
    "    \n",
    "    # ngramed_x : [ URL1[word1[h,t,t,p], word2[:], word3[/], word4[n,a,v,e,r]], URl2[]]\n",
    "    # ngramed_id_x 는 위에걸 숫자 id로 바꾼 것을 말한다. \n",
    "    # worded_id_x 는 한 URL을 특수문자(ex: / , :) 하나를 하나의 word로 포함시켜서, 이 word 목록에다가 겹치지 않게 해서 id를 mapping시킨걸 의미 \n",
    "    # words_dict 는 이걸 mapping 할 수 있게 해주는 dictionary이다. \n",
    "    return ngramed_id_x, ngrams_dict, worded_id_x, words_dict \n",
    "\n",
    "# data.max_len_words 는 단어의 최디 길이가 아니라, URL을 단어 단위로 표현할 때, 최대 몇개까지의 단어로 이루어질 수 있는가를 표현.\n",
    "min_freq = 1\n",
    "if min_freq > 0: \n",
    "    x1, word_reverse_dict = get_word_vocab(urls, 20, min_freq) \n",
    "    high_freq_words = sorted(list(word_reverse_dict.values()))\n",
    "    print(\"Number of words with freq >={}: {}\".format(min_freq, len(high_freq_words)))  \n",
    "\n",
    "x, word_reverse_dict = get_word_vocab(urls, 200) \n",
    "word_x = get_words(x, word_reverse_dict, 1, urls)    \n",
    "    \n",
    "ngramed_id_x, ngrams_dict, worded_id_x, words_dict = ngram_id_x(word_x, 20, high_freq_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[19, 18, 14, 14, 7, 3, 8],\n",
       "  [19, 22, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 25, 21, 17, 10, 24, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 26, 1, 5, 8]],\n",
       " [[19, 18, 14, 14, 7, 3, 8],\n",
       "  [19, 22, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 13, 1, 26, 3, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 21, 18, 25, 9, 21, 4, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 26, 1, 5, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 21, 16, 9, 21, 4, 8],\n",
       "  [19, 23, 8],\n",
       "  [19, 13, 15, 8]],\n",
       " [[19, 18, 14, 14, 7, 3, 8],\n",
       "  [19, 22, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 7, 18, 16, 3, 18, 16, 25, 27, 8]],\n",
       " [[19, 15, 14, 7, 8],\n",
       "  [19, 22, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 13, 15, 3, 13, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 11, 1, 6, 25, 27, 20, 21, 16, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 20, 6, 18, 10, 10, 8],\n",
       "  [19, 12, 8],\n",
       "  [19, 26, 1, 5, 8],\n",
       "  [19, 2, 8],\n",
       "  [19, 9, 1, 17, 10, 8]]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramed_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o': 1,\n",
       " '/': 2,\n",
       " 's': 3,\n",
       " 'b': 4,\n",
       " 'm': 5,\n",
       " 'u': 6,\n",
       " 'p': 7,\n",
       " '>': 8,\n",
       " 'l': 9,\n",
       " 'e': 10,\n",
       " 'y': 11,\n",
       " '.': 12,\n",
       " 'd': 13,\n",
       " 't': 14,\n",
       " 'f': 15,\n",
       " 'i': 16,\n",
       " 'v': 17,\n",
       " 'h': 18,\n",
       " '<': 19,\n",
       " 'j': 20,\n",
       " 'a': 21,\n",
       " ':': 22,\n",
       " '?': 23,\n",
       " 'r': 24,\n",
       " 'n': 25,\n",
       " 'c': 26,\n",
       " 'g': 27}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6, 2, 2, 3, 4, 5],\n",
       " [1, 6, 2, 2, 3, 4, 3, 4, 5, 2, 3, 7, 3],\n",
       " [1, 6, 2, 2, 3]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worded_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https': 1, '/': 2, '<UNKNOWN>': 3, '.': 4, 'com': 5, ':': 6, '?': 7}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 걍 URL을 한 글자씩 잘라가지고, char_dict(한 글자당 mapping되어 있는 숫자 id) 에서 찾아가지고 \n",
    "# character 대신에 그냥 숫자 id로 표현하는 것임.\n",
    "def char_id_x(urls, char_dict, max_len_chars): \n",
    "    chared_id_x = []\n",
    "    for url in urls: \n",
    "        url = list(url) \n",
    "        url_in_char_id = []\n",
    "        l = min(len(url), max_len_chars)\n",
    "        \n",
    "        # 길어봤자 200개까지만 truncated할거라서, 거기까지만 idx를 \n",
    "        for i in range(l): \n",
    "            c = url[i] \n",
    "            \n",
    "            # 걍 그 글자(character) 에 해당하는 숫자 id를 알고 싶은 것임. \n",
    "            try:\n",
    "                c_id = char_dict[c] \n",
    "            except KeyError:\n",
    "                c_id = 0\n",
    "            url_in_char_id.append(c_id) \n",
    "        chared_id_x.append(url_in_char_id) \n",
    "    return chared_id_x \n",
    "\n",
    "chars_dict = ngrams_dict\n",
    "\n",
    "\n",
    "# max_len_chars : 한 URL이 character를 가질 수 있는 최대 갯수 : 200\n",
    "chared_id_x = char_id_x(urls, chars_dict, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 14, 14, 7, 3, 22, 2, 2, 25, 21, 17, 10, 24, 12, 26, 1, 5],\n",
       " [18,\n",
       "  14,\n",
       "  14,\n",
       "  7,\n",
       "  3,\n",
       "  22,\n",
       "  2,\n",
       "  2,\n",
       "  13,\n",
       "  1,\n",
       "  26,\n",
       "  3,\n",
       "  12,\n",
       "  21,\n",
       "  18,\n",
       "  25,\n",
       "  9,\n",
       "  21,\n",
       "  4,\n",
       "  12,\n",
       "  26,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  21,\n",
       "  16,\n",
       "  9,\n",
       "  21,\n",
       "  4,\n",
       "  23,\n",
       "  13,\n",
       "  15],\n",
       " [18, 14, 14, 7, 3, 22, 2, 2, 7, 18, 16, 3, 18, 16, 25, 27],\n",
       " [15,\n",
       "  14,\n",
       "  7,\n",
       "  22,\n",
       "  2,\n",
       "  2,\n",
       "  13,\n",
       "  15,\n",
       "  3,\n",
       "  13,\n",
       "  12,\n",
       "  11,\n",
       "  1,\n",
       "  6,\n",
       "  25,\n",
       "  27,\n",
       "  20,\n",
       "  21,\n",
       "  16,\n",
       "  12,\n",
       "  20,\n",
       "  6,\n",
       "  18,\n",
       "  10,\n",
       "  10,\n",
       "  12,\n",
       "  26,\n",
       "  1,\n",
       "  5,\n",
       "  2,\n",
       "  9,\n",
       "  1,\n",
       "  17,\n",
       "  10]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chared_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mal/Ben split: 9/9\n",
      "Train Mal/Ben split: 8/8\n",
      "Test Mal/Ben split: 1/1\n",
      "Train/Test split: 16/2\n",
      "Train/Test split: 16/2\n"
     ]
    }
   ],
   "source": [
    "from tflearn.data_utils import to_categorical \n",
    "'''\n",
    "pos_x 는 URL이 +인 idx 번호가 들어있는 배열이다. \n",
    "neg_x 는 URL이 -인 idx 번호가 들어있는 배열이다. \n",
    "'''\n",
    "\n",
    "\n",
    "labels = [-1,-1,1,1,1,-1,1,-1,-1,1,1,1,1,-1,-1,-1,1,-1]\n",
    "pos_x = []\n",
    "neg_x = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i] \n",
    "    if label == 1: \n",
    "        pos_x.append(i)\n",
    "    else: \n",
    "        neg_x.append(i)\n",
    "print(\"Overall Mal/Ben split: {}/{}\".format(len(pos_x), len(neg_x)))\n",
    "pos_x = np.array(pos_x) \n",
    "neg_x = np.array(neg_x) \n",
    "\n",
    "\n",
    "def prep_train_test(pos_x, neg_x, dev_pct): \n",
    "    np.random.seed(10) \n",
    "    \n",
    "    # 말그대로 주어진 +인 idx목록에서 순서를 뒤죽박죽 섞는다.(permutation역할) \n",
    "    shuffle_indices=np.random.permutation(np.arange(len(pos_x)))\n",
    "    \n",
    "    # idx가 섞였다. (1,2,3,4 이런식의 indices가 나오지 않는다.)\n",
    "    pos_x_shuffled = pos_x[shuffle_indices]\n",
    "    dev_idx = -1 * int(dev_pct * float(len(pos_x)))\n",
    "    \n",
    "    # 어차피 지금 순서가 뒤죽박죽으로 섞여있는 상태니깐, 처음부터 그 비율에 맞는 수까지만 고른다. \n",
    "    pos_train = pos_x_shuffled[:dev_idx]\n",
    "    pos_test = pos_x_shuffled[dev_idx:]\n",
    "\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices=np.random.permutation(np.arange(len(neg_x)))\n",
    "    neg_x_shuffled = neg_x[shuffle_indices]\n",
    "    dev_idx = -1 * int(dev_pct * float(len(neg_x)))\n",
    "    neg_train = neg_x_shuffled[:dev_idx]\n",
    "    neg_test = neg_x_shuffled[dev_idx:] \n",
    "\n",
    "    # 그냥 idx목록들이 담겨져있다? \n",
    "    x_train = np.array(list(pos_train) + list(neg_train))\n",
    "    y_train = len(pos_train)*[1] + len(neg_train)*[0]\n",
    "    x_test = np.array(list(pos_test) + list(neg_test))\n",
    "    y_test = len(pos_test)*[1] + len(neg_test)*[0]\n",
    "\n",
    "    # y를 categorical 하게 해주는게 무슨 의미?\n",
    "    y_train = to_categorical(y_train, nb_classes=2)\n",
    "    y_test = to_categorical(y_test, nb_classes=2) \n",
    "\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(x_train)))\n",
    "    x_train = x_train[shuffle_indices]\n",
    "    y_train = y_train[shuffle_indices]\n",
    "\n",
    "    np.random.seed(10) \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(x_test)))\n",
    "    x_test = x_test[shuffle_indices]\n",
    "    y_test = y_test[shuffle_indices] \n",
    "    \n",
    "    print(\"Train Mal/Ben split: {}/{}\".format(len(pos_train), len(neg_train)))\n",
    "    print(\"Test Mal/Ben split: {}/{}\".format(len(pos_test), len(neg_test)))\n",
    "    print(\"Train/Test split: {}/{}\".format(len(y_train), len(y_test)))\n",
    "    print(\"Train/Test split: {}/{}\".format(len(x_train), len(x_test)))\n",
    "\n",
    "    # shuffled_indices로 한번에 하는거라 그 순서는 잘 mapping이 된다.\n",
    "    # x쪽 데이터는 label의 idx이고, test 해당 label의 결과를 one-hot encoding한 것이다. \n",
    "    return x_train, y_train, x_test, y_test \n",
    "\n",
    "# x_train,\n",
    "x_train, y_train, x_test, y_test = prep_train_test(pos_x, neg_x, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_x_shuffled = pos_x[shuffle_indices]\n",
    "pos_train = pos_x_shuffled[:dev_idx]\n",
    "shuffle_indices=np.random.permutation(np.arange(len(neg_x)))\n",
    "neg_x_shuffled = neg_x[shuffle_indices]\n",
    "dev_idx = -1 * int(0.2 * float(len(neg_x)))\n",
    "neg_train = neg_x_shuffled[:dev_idx]\n",
    "y_train = len(pos_train)*[1] + len(neg_train)*[0]\n",
    "y_train\n",
    "# y_train = to_categorical(y_train, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = to_categorical(y_train, nb_classes=2)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  4, 17,  5, 13, 14,  7,  1,  0, 15])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(list(pos_train) + list(neg_train))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_idx = -1 * int(0.2 * float(len(neg_x)))\n",
    "dev_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.2 * float(len(neg_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-16b2f513ad92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_ngramed_id_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_train_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngramed_id_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngramed_id_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-16b2f513ad92>\u001b[0m in \u001b[0;36mget_ngramed_id_x\u001b[0;34m(x_idxs, ngramed_id_x)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moutput_ngramed_id_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0moutput_ngramed_id_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngramed_id_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_ngramed_id_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# 걍 train에 있는 idx에 맞게 ngram 된것 mapping 시켜서 맞춰주는것 \n",
    "def get_ngramed_id_x(x_idxs, ngramed_id_x): \n",
    "    output_ngramed_id_x = [] \n",
    "    for idx in x_idxs:  \n",
    "        output_ngramed_id_x.append(ngramed_id_x[idx])\n",
    "    return output_ngramed_id_x\n",
    "\n",
    "x_train_char = get_ngramed_id_x(x_train, ngramed_id_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yield ? \n",
    "\n",
    "batch 로 만들때 자주 쓰인다. \n",
    "엄청나게 방대한 데이터를 batch로 쪼개면 그게 다 memory에 올라가 있게 된다.      \n",
    "그런데, 이렇게 yield로 하면 필요할 때마다 수행을 하게 된다. batch 별로 순환하게 될텐데, 모두 순환해서 batch들의 묵음을 전달하는 것이 아니라, 첫 batch 하고 나면, yield 되서 해당 batch에 대한 연산을 한다.     \n",
    "그리고 나서, 다시 yield가 있는 함수로 넘어가서 두번째 batch를 계산하게 된다.      \n",
    "\n",
    "yield가 있는 함수를 처음부터 싹다 계산해서 결과값을 얻어내는 것이 아니라, 그때그때마다 계산해서 값을 얻어내는 방식이다. \n",
    "\n",
    "어떤 iterable을 만드는데, iterable 내의 각 원소를 접근할 때는 그냥 순차적으로 간다는 말이다. \n",
    "한 번에 다 memory에 올리는 것이 아니라..      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Start\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Function End\n"
     ]
    }
   ],
   "source": [
    "def number_generator(n):\n",
    "    print(\"Function Start\")\n",
    "    while n <6:\n",
    "        yield n\n",
    "        n+= 1\n",
    "    print(\"Function End\")\n",
    "     \n",
    "if __name__== \"__main__\":\n",
    "    for i in number_generator(0):\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Start\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e28ee843dbe8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumber_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def number_generator(n):\n",
    "    print(\"Function Start\")\n",
    "    while n <6:\n",
    "        n+= 1\n",
    "        return n\n",
    "    print(\"Function End\")\n",
    "     \n",
    "if __name__== \"__main__\":\n",
    "    for idx in number_generator(0):\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

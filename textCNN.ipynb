{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "char_ngram_vocab_size : len(ngrams_dict) +1 => 글자가 몇 개 있는지\n",
    "word_ngram_vocab_size : len(words_dict) + 1 => 단어가 몇 개 있는지\n",
    "\n",
    "chars_dict = ngrams_dict 로 두었다. 왜냐하면, 지금 ngram 크기를 1로 두어서 그럼. \n",
    "\n",
    "char_vocab_size = len(chars_dict)+1 => 따라서 chars_dict도 ngram으로 한거랑 같게 되는 것임. \n",
    "\n",
    "'''\n",
    "class TextCNN(object): \n",
    "    def __init__(self, char_ngram_vocab_size, word_ngram_vocab_size, char_vocab_size, \\\n",
    "        word_seq_len, char_seq_len, embedding_size, l2_reg_lambda=0, \\\n",
    "        filter_sizes=[3,4,5,6], mode=0): \n",
    "        if mode == 4 or mode == 5: \n",
    "            # 이 경우에는 char-level로 embedding 시키는것도 있어야한다. \n",
    "            # 이 때 char는 embedding된 vector를 이용하기 때문에 size가 다음과 같다. \n",
    "            self.input_x_char = tf.placeholder(tf.int32, [None, None, None], name=\"input_x_char\")\n",
    "            self.input_x_char_pad_idx = tf.placeholder(tf.float32, [None, None, None, embedding_size], name=\"input_x_char_pad_idx\")\n",
    "        if mode == 4 or mode == 5 or mode == 2 or mode == 3: \n",
    "            self.input_x_word = tf.placeholder(tf.int32, [None, None], name=\"input_x_word\")\n",
    "        if mode == 1 or mode == 3 or mode == 5: \n",
    "            self.input_x_char_seq = tf.placeholder(tf.int32, [None, None], name=\"input_x_char_seq\") \n",
    "\n",
    "        # y는 label이니깐, None 에다가 마지막 차원은 2이다.     \n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 2], name=\"input_y\") \n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\") \n",
    "\n",
    "        l2_loss = tf.constant(0.0) \n",
    "        with tf.name_scope(\"embedding\"): \n",
    "            \n",
    "            # mode가 4,5 일 때는 word를 다시 character level로 나누는거 같다. 그래서 unseen word 에 대한 pattern을 학습시키는 것 같다. \n",
    "            # 그래서 word를 다시, ngram으로 나눠서 하나의 word를 표현하는 ngram단위로 다시 표현하려고 character-level 이라는게 붙는것같다. \n",
    "            if mode == 4 or mode == 5: \n",
    "                # char-level일 때, character를 나타내는 embedding vector이다. \n",
    "                self.char_w = tf.Variable(tf.random_uniform([char_ngram_vocab_size, embedding_size], -1.0, 1.0), name=\"char_emb_w\") \n",
    "            if mode == 2 or mode == 3 or mode == 4 or mode == 5: \n",
    "                self.word_w = tf.Variable(tf.random_uniform([word_ngram_vocab_size, embedding_size], -1.0, 1.0), name=\"word_emb_w\")\n",
    "            if mode == 1 or mode == 3 or mode == 5: \n",
    "                \n",
    "                # 딱 글자 단위로만 잘랐을 때, 그 개개의 character 마다의 embedding 값\n",
    "                \n",
    "                self.char_seq_w = tf.Variable(tf.random_uniform([char_vocab_size, embedding_size], -1.0, 1.0), name=\"char_seq_emb_w\")        \n",
    "     \n",
    "            if mode == 4 or mode == 5: \n",
    "            \n",
    "                # input으로 들어온 char를 w즉 임베딩된 table에서 맞는걸 찾아서 주는듯? \n",
    "                # W가 각 char 별로 embedding 값이 들어있다. 이 때, 내부적으로 어떤 char가 어떤 embedding 값에 매칭되는지를 idx를 내부적으로 정한다. \n",
    "                # 그 다음에 input이 들어오면 그것에 해당하는 idx를 구해서, idx를 중심으로 embedding table에 접근해서 값을 가져오는 듯 하다. \n",
    "                self.embedded_x_char = tf.nn.embedding_lookup(self.char_w, self.input_x_char)\n",
    "                \n",
    "                # 왜 input_x_char_pad_idx랑 곱하지? \n",
    "                self.embedded_x_char = tf.multiply(self.embedded_x_char, self.input_x_char_pad_idx)\n",
    "            \n",
    "            if mode == 2 or mode == 3 or mode == 4 or mode == 5: \n",
    "                self.embedded_x_word = tf.nn.embedding_lookup(self.word_w, self.input_x_word) \n",
    "            if mode == 1 or mode == 3 or mode == 5: \n",
    "                self.embedded_x_char_seq = tf.nn.embedding_lookup(self.char_seq_w, self.input_x_char_seq)  \n",
    "\n",
    "            if mode == 4 or mode == 5: \n",
    "                self.sum_ngram_x_char = tf.reduce_sum(self.embedded_x_char, 2)         \n",
    "                self.sum_ngram_x = tf.add(self.sum_ngram_x_char, self.embedded_x_word)\n",
    "\n",
    "            if mode == 4 or mode == 5: \n",
    "                self.sum_ngram_x_expanded = tf.expand_dims(self.sum_ngram_x, -1) \n",
    "            if mode == 2 or mode == 3: \n",
    "                self.sum_ngram_x_expanded = tf.expand_dims(self.embedded_x_word, -1) \n",
    "            if mode == 1 or mode == 3 or mode == 5: \n",
    "                self.char_x_expanded = tf.expand_dims(self.embedded_x_char_seq, -1) \n",
    "        \n",
    "        ########################### WORD CONVOLUTION LAYER ################################\n",
    "        if mode == 2 or mode == 3 or mode == 4 or mode == 5: \n",
    "            pooled_x = []\n",
    "\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.name_scope(\"conv_maxpool_%s\" % filter_size): \n",
    "                    filter_shape = [filter_size, embedding_size, 1, 256]\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b\") \n",
    "                    w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"w\") \n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.sum_ngram_x_expanded,  # weight를 적용할, 주어진 것. \n",
    "                        w,\n",
    "                        strides = [1,1,1,1],\n",
    "                        padding = \"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv,b), name=\"relu\") \n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, word_seq_len - filter_size + 1, 1, 1],\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"pool\") \n",
    "                    pooled_x.append(pooled) \n",
    "        \n",
    "            num_filters_total = 256 * len(filter_sizes) \n",
    "            self.h_pool = tf.concat(pooled_x, 3)\n",
    "            self.x_flat = tf.reshape(self.h_pool, [-1, num_filters_total], name=\"pooled_x\")  \n",
    "            self.h_drop = tf.nn.dropout(self.x_flat, self.dropout_keep_prob, name=\"dropout_x\") \n",
    "\n",
    "        ########################### CHAR CONVOLUTION LAYER ###########################\n",
    "        if mode == 1 or mode == 3 or mode == 5: \n",
    "            pooled_char_x = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                with tf.name_scope(\"char_conv_maxpool_%s\" % filter_size):\n",
    "                    filter_shape = [filter_size, embedding_size, 1, 256]\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b\")\n",
    "                    w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"w\")\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.char_x_expanded,\n",
    "                        w,\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv,b), name=\"relu\")\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, char_seq_len - filter_size + 1, 1, 1],\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"pool\")\n",
    "                    pooled_char_x.append(pooled) \n",
    "            num_filters_total = 256*len(filter_sizes) \n",
    "            self.h_char_pool = tf.concat(pooled_char_x, 3)\n",
    "            self.char_x_flat = tf.reshape(self.h_char_pool, [-1, num_filters_total], name=\"pooled_char_x\")\n",
    "            self.char_h_drop = tf.nn.dropout(self.char_x_flat, self.dropout_keep_prob, name=\"dropout_char_x\")\n",
    "        \n",
    "        ############################### CONCAT WORD AND CHAR BRANCH ############################\n",
    "        if mode == 3 or mode == 5: \n",
    "            with tf.name_scope(\"word_char_concat\"): \n",
    "                ww = tf.get_variable(\"ww\", shape=(num_filters_total, 512), initializer=tf.contrib.layers.xavier_initializer())\n",
    "                bw = tf.Variable(tf.constant(0.1, shape=[512]), name=\"bw\") \n",
    "                l2_loss += tf.nn.l2_loss(ww) \n",
    "                l2_loss += tf.nn.l2_loss(bw) \n",
    "                word_output = tf.nn.xw_plus_b(self.h_drop, ww, bw)\n",
    "\n",
    "                wc = tf.get_variable(\"wc\", shape=(num_filters_total, 512), initializer=tf.contrib.layers.xavier_initializer())\n",
    "                bc = tf.Variable(tf.constant(0.1, shape=[512]), name=\"bc\") \n",
    "                l2_loss += tf.nn.l2_loss(wc)\n",
    "                l2_loss += tf.nn.l2_loss(bc)\n",
    "                char_output = tf.nn.xw_plus_b(self.char_h_drop, wc, bc) \n",
    "            \n",
    "                self.conv_output = tf.concat([word_output, char_output], 1)              \n",
    "        elif mode == 2 or mode == 4: \n",
    "            self.conv_output = self.h_drop \n",
    "        elif mode == 1: \n",
    "            self.conv_output = self.char_h_drop        \n",
    "\n",
    "        ################################ RELU AND FC ###################################\n",
    "        with tf.name_scope(\"output\"): \n",
    "            w0 = tf.get_variable(\"w0\", shape=[1024, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b0 = tf.Variable(tf.constant(0.1, shape=[512]), name=\"b0\") \n",
    "            l2_loss += tf.nn.l2_loss(w0) \n",
    "            l2_loss += tf.nn.l2_loss(b0) \n",
    "            output0 = tf.nn.relu(tf.matmul(self.conv_output, w0) + b0)\n",
    "            \n",
    "            w1 = tf.get_variable(\"w1\", shape=[512, 256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "            b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b1\") \n",
    "            l2_loss += tf.nn.l2_loss(w1) \n",
    "            l2_loss += tf.nn.l2_loss(b1) \n",
    "            output1 = tf.nn.relu(tf.matmul(output0, w1) + b1)\n",
    "            \n",
    "            w2 = tf.get_variable(\"w2\", shape=[256,128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.constant(0.1, shape=[128]), name=\"b2\") \n",
    "            l2_loss += tf.nn.l2_loss(w2) \n",
    "            l2_loss += tf.nn.l2_loss(b2) \n",
    "            output2 = tf.nn.relu(tf.matmul(output1, w2) + b2) \n",
    "            \n",
    "            w = tf.get_variable(\"w\", shape=(128, 2), initializer=tf.contrib.layers.xavier_initializer()) \n",
    "            b = tf.Variable(tf.constant(0.1, shape=[2]), name=\"b\") \n",
    "            l2_loss += tf.nn.l2_loss(w) \n",
    "            l2_loss += tf.nn.l2_loss(b) \n",
    "            \n",
    "            self.scores = tf.nn.xw_plus_b(output2, w, b, name=\"scores\") \n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\") \n",
    "\n",
    "        with tf.name_scope(\"loss\"): \n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y) \n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"): \n",
    "            correct_preds = tf.equal(self.predictions, tf.argmax(self.input_y, 1)) \n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_preds, \"float\"), name=\"accuracy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL들을 그냥 character level로 해서, id로 변환한 것 \n",
    "chared_id_x = char_id_x(urls, chars_dict, FLAGS[\"data.max_len_chars\"])\n",
    "\n",
    "# character의 id로 변환한 걸 idx에 매칭시켜서 얻어낸 결과 \n",
    "x_train_char_seq = get_ngramed_id_x(x_train, chared_id_x)\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True): \n",
    "    data = np.array(data) \n",
    "    data_size = len(data) \n",
    "    \n",
    "    # 1 epoch 당 batch의 갯수 (1batch당 128개씩 한다고 했으니깐, 그런 batch의 갯수는?) \n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1 \n",
    "    for epoch in range(num_epochs): \n",
    "        if shuffle: \n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size)) \n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else: \n",
    "            shuffled_data = data \n",
    "        for batch_num in range(num_batches_per_epoch): \n",
    "            start_idx = batch_num * batch_size \n",
    "            end_idx = min((batch_num+1) * batch_size, data_size)\n",
    "            \n",
    "            # 현재 섞여 있는 전체데이터에서, batch의 시작 idx, batch의 끝 idx를 찾아서 그만큼씩을 호출하는 것임. \n",
    "            yield shuffled_data[start_idx:end_idx]\n",
    "\n",
    "\n",
    "'''\n",
    "x_train_char  : URL의 각각의 단어가 ngram되어서, ngram id로 mapping되어서 나온형태 (char랑 별 다를건 없음)\n",
    "x_train_word  : URL의 각 단어를 그냥 단어자체의 id로 mapping 시켜서 나온 형태 \n",
    "x_train_char_seq : URL을 단어로 쪼개는 것도 아니고, 그냥 글자 단위 하나하나로 봐서 character에 대응되는 id로 mapping 시킨 것\n",
    "'''\n",
    "def make_batches(x_train_char_seq, x_train_word, x_train_char, y_train, batch_size, nb_epochs, shuffle=False):\n",
    "    if FLAGS[\"model.emb_mode\"] == 1:  \n",
    "        batch_data = list(zip(x_train_char_seq, y_train))\n",
    "    elif FLAGS[\"model.emb_mode\"] == 2:  \n",
    "        batch_data = list(zip(x_train_word, y_train))\n",
    "    elif FLAGS[\"model.emb_mode\"] == 3:  \n",
    "        batch_data = list(zip(x_train_char_seq, x_train_word, y_train))\n",
    "    elif FLAGS[\"model.emb_mode\"] == 4:\n",
    "         batch_data = list(zip(x_train_char, x_train_word, y_train))\n",
    "    elif FLAGS[\"model.emb_mode\"] == 5:  \n",
    "        batch_data = list(zip(x_train_char, x_train_word, x_train_char_seq, y_train))\n",
    "        \n",
    "    # 말 그대로 batch 별로 뽑아내서 iteration 시켜주는 것이다. \n",
    "    # 여기에 batch는 하나의 batch이다. \n",
    "    batches = batch_iter(batch_data, batch_size, nb_epochs, shuffle)\n",
    "\n",
    "    if nb_epochs > 1: \n",
    "        nb_batches_per_epoch = int(len(batch_data)/batch_size)\n",
    "        \n",
    "        # nb_batches_per_epoch 는 그 해당 epoch에서 몇번째 batch 인지를 나타낸다. \n",
    "        # 총 몇번째 batch인지 (epoch 통틀어서) 를 nb_batch로 표현한다. \n",
    "        if len(batch_data)%batch_size != 0:\n",
    "            nb_batches_per_epoch += 1\n",
    "        nb_batches = int(nb_batches_per_epoch * nb_epochs)\n",
    "        return batches, nb_batches_per_epoch, nb_batches\n",
    "    else:\n",
    "        return batches \n",
    "    \n",
    "    \n",
    "train_batches, nb_batches_per_epoch, nb_batches = make_batches(x_train_char_seq, x_train_word, x_train_char, y_train, 128, 5, True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x 는 batch인데 mode에 따라 다음과 같이 구성되어 있다. \n",
    "\n",
    "pad_idx 가 char-level로 word를 나타내는 것이다. ngram해서. \n",
    "\n",
    "mode = 1 [x_char_seq]\n",
    "mode = 2 [x_word ]\n",
    "mode = 3 [x_char_seq, x_word]\n",
    "mode = 4 [x_word, x_char, x_char_pad_idx]\n",
    "mode = 5 [x_char_seq, x_word, x_char, x_char_pad_idx]\n",
    "'''\n",
    "\n",
    "def train_dev_step(x, y, emb_mode, is_train=True):\n",
    "    if is_train: \n",
    "        p = 0.5\n",
    "    else: \n",
    "        p = 1.0\n",
    "    if emb_mode == 1: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}  \n",
    "    elif emb_mode == 2: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_word: x[0],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 3: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_x_word: x[1],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 4: \n",
    "        feed_dict = {\n",
    "            cnn.input_x_word: x[0],\n",
    "            cnn.input_x_char: x[1],\n",
    "            cnn.input_x_char_pad_idx: x[2],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    elif emb_mode == 5:  \n",
    "        feed_dict = {\n",
    "            cnn.input_x_char_seq: x[0],\n",
    "            cnn.input_x_word: x[1],\n",
    "            cnn.input_x_char: x[2],\n",
    "            cnn.input_x_char_pad_idx: x[3],\n",
    "            cnn.input_y: y,\n",
    "            cnn.dropout_keep_prob: p}\n",
    "    if is_train:\n",
    "        _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "    else: \n",
    "        step, loss, acc = sess.run([global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "    return step, loss, acc\n",
    "\n",
    "\n",
    "# 실제로 학습이 진행되는 부분인듯. \n",
    "\n",
    "for idx in it:\n",
    "    batch = next(train_batches)  # train_batches가 generator, iterator이므로 그때그때마다 next()로해서 batch를 구해온다. \n",
    "    x_batch, y_batch = prep_batches(batch) \n",
    "    step, loss, acc = train_dev_step(x_batch, y_batch, emb_mode=FLAGS[\"model.emb_mode\"], is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 batch이다.  \n",
    "'''\n",
    "mode = 1 : only character-based CNN, \n",
    "mode = 2 : only word-based CNN, \n",
    "mode = 3 : character and word CNN, \n",
    "mode = 4 : character-level word CNN, \n",
    "mode = 5 : character and character-level word \n",
    "\n",
    "\n",
    "urls 는 x_char_seq, 혹은 x_word가 들어가는데, 이건 url을 character별로 쪼개서, character에 대응하는 숫자 id로만 mapping 킨것\n",
    "max_d1 은 한 URL에 들어갈 수 있는 최대 character 갯수 \n",
    "'''\n",
    "def pad_seq_in_word(urls, max_d1=0, embedding_size=128):\n",
    "    if max_d1 == 0: \n",
    "        url_lens = [len(url) for url in urls]\n",
    "        max_d1 = max(url_lens)\n",
    "        \n",
    "    # 일단 최대길이를 기준으로 해서 0으로 다 채운다. [[0,0,0,0]. [0,0,0,0], [0,0,0,0] ] 이런식으로..     \n",
    "    pad_urls = np.zeros((len(urls), max_d1))\n",
    "    #pad_idx = np.zeros((len(urls), max_d1, embedding_size))\n",
    "    #pad_vec = [1 for i in range(embedding_size)]\n",
    "    \n",
    "    # 일단 0으로 padding 된 자리에다가, \n",
    "    for d0 in range(len(urls)): \n",
    "        url = urls[d0]\n",
    "        for d1 in range(len(url)): \n",
    "            if d1 < max_d1: \n",
    "                pad_urls[d0,d1] = url[d1]\n",
    "                #pad_idx[d0,d1] = pad_vec \n",
    "    return pad_urls \n",
    "\n",
    "'''\n",
    "pad_seq가 사용되는 경우는 emb_mode가 4,5일 때이다. char-level word인 경우 \n",
    "urls에 x_char가 들어가는데... x_chars 는 URL의 단어별로 쪼개고, 단어를 ngram단위로 쪼개고 난 다음에, id로 mapping 시킨것. \n",
    "\n",
    "이 함수가 하고자 하는 바는, word 단위에서 다시 ngram으로 쪼개서, 그 ngram string이랑, 해당 vector를 적는것 \n",
    "max_d1 = .max_len_words => 한 URL에 최대로 들어갈 수 있는 단어의 갯수 (200) \n",
    "max_d2 = max_len_subwords => 한 단어에 최대로 들어갈 수 있는 단어의 갯수 (20)\n",
    "'''\n",
    "def pad_seq(urls, max_d1=0, max_d2=0, embedding_size=128): \n",
    "    \n",
    "    # 정해져있지 않은경우, 찾는과정. \n",
    "    if max_d1 == 0 and max_d2 == 0: \n",
    "        for url in urls: \n",
    "            if len(url) > max_d1: \n",
    "                max_d1 = len(url) \n",
    "            for word in url: \n",
    "                if len(word) > max_d2: \n",
    "                    max_d2 = len(word) \n",
    "                    \n",
    "    # pad_idx 는 URL 내 단어의 character들을 받는다. 근데 그 character는 embedding size의 vec로 표현된다.\n",
    "    # 따라서 pad_idx의 차원은 URL 갯수 * URL내 최대 단어갯수 * 단어 내 최대 글자갯수 * 글자의 embedding vector 수 \n",
    "    pad_idx = np.zeros((len(urls), max_d1, max_d2, embedding_size))\n",
    "    pad_urls = np.zeros((len(urls), max_d1, max_d2))\n",
    "    \n",
    "    # 해당 character가 존재함을 표시하기 위해서 [1,1,1,1,1] 과 같은 vector로 표현한다. \n",
    "    pad_vec = [1 for i in range(embedding_size)]\n",
    "    for d0 in range(len(urls)): \n",
    "        url = urls[d0]\n",
    "        for d1 in range(len(url)): \n",
    "            \n",
    "            if d1 < max_d1: \n",
    "                # word는 ngram으로 이루어진 단어 => emb_mode가 4,5 일때.. \n",
    "                word = url[d1]\n",
    "                \n",
    "                # 한 단어 내에서 character 나, ngram에 접근할 때는. 한 단어가 최대 몇개의 character로 이뤄지는지를 알아야함.\n",
    "                for d2 in range(len(word)): \n",
    "                    if d2 < max_d2: \n",
    "                        # 한 단어의 character를 넣는다. . \n",
    "                        pad_urls[d0,d1,d2] = word[d2]  # 그 string으로 들어간다. \n",
    "                        \n",
    "                        # pad\n",
    "                        pad_idx[d0,d1,d2] = pad_vec  # 해당 string이 embedding vector로 들어간다. \n",
    "                        \n",
    "    # pad_urls 는 단                \n",
    "    return pad_urls, pad_idx\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "x_char_seq 는 URL의 character를 숫자 id로 매칭시켜서 얻어낸 결과임 \n",
    "\n",
    "'''\n",
    "def prep_batches(batch):\n",
    "    \n",
    "    # batch 는 다음과 같이 구성되어 있따. x_train_char, x_train_word, x_train_char_seq, y_train\n",
    "    if FLAGS[\"model.emb_mode\"] == 1:\n",
    "        x_char_seq, y_batch = zip(*batch)\n",
    "    elif FLAGS[\"model.emb_mode\"] == 2:\n",
    "        x_word, y_batch = zip(*batch)\n",
    "    elif FLAGS[\"model.emb_mode\"] == 3:\n",
    "        x_char_seq, x_word, y_batch = zip(*batch)\n",
    "    elif FLAGS[\"model.emb_mode\"] == 4:\n",
    "        x_char, x_word, y_batch = zip(*batch)\n",
    "    elif FLAGS[\"model.emb_mode\"] == 5:\n",
    "        x_char, x_word, x_char_seq, y_batch = zip(*batch)\n",
    "\n",
    "    x_batch = []\n",
    "    \n",
    "    # batch를 통해서 얻어낸걸, 모델에 투입하기 전에, 그 길이에 맞춰서 \n",
    "    if FLAGS[\"model.emb_mode\"] in [1, 3, 5]:\n",
    "        #  URL을 그냥 하나의 char단위로만 쭉 펼쳤을 때도 고려해야 하므로.\n",
    "        x_char_seq = pad_seq_in_word(x_char_seq, FLAGS[\"data.max_len_chars\"])\n",
    "        x_batch.append(x_char_seq)\n",
    "    if FLAGS[\"model.emb_mode\"] in [2, 3, 4, 5]:\n",
    "        \n",
    "        x_word = pad_seq_in_word(x_word, FLAGS[\"data.max_len_words\"])\n",
    "        x_batch.append(x_word)\n",
    "    if FLAGS[\"model.emb_mode\"] in [4, 5]:\n",
    "        \n",
    "        # x_char 는 URL을 다시 word 단위로 쪼개고, word에서 ngram으로 나눠 들어가서 살펴봤을 때이다. \n",
    "        # 무튼 x_char는 padding으로 채워진 값이면서, size가[URL 갯수, URL당 word갯수, word당 maximum 글자갯수]\n",
    "        x_char, x_char_pad_idx = pad_seq(x_char, FLAGS[\"data.max_len_words\"], FLAGS[\"data.max_len_subwords\"], FLAGS[\"model.emb_dim\"])\n",
    "        \n",
    "        # emb_mode가 4,5 인 경우에는 아래 경우들도 추가해야한다. \n",
    "        x_batch.extend([x_char, x_char_pad_idx])\n",
    "        \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
